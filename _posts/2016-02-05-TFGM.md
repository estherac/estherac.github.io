---
layout: post
title: Projects for 2016
link: 
link-alt: 
category: teaching
description: TFG and TFM for the students (UC3M)
article: yes

---

{:.no_toc}

* This line will be replaced by the ToC, excluding the "Contents" header
{:toc}

# Prerequisites

 * C++/Python development skills, also Arduino would be helpful.
 * Git/SVN versioning systems (not mandatory, they can be learned on the go).
 * Basic knowledge of 3D printers, control engineering, basic AI, computer vision, robotics would be helpful.
 * ROS knowledge would be helpful (not mandatory, ROS indigo version).
 * Linux knowledge would be helpful (all the robots code runs on Ubuntu 14.04).
 * English skills would be helpful (research articles are usually in this language).

# Expected results

 * Things you will learn (if you do not know previously):
   * ROS
   * robotics perception
   * robotics control
   * standard computer science skills 
   * machine learning (depending on project)
   * optimization techniques (depending on project)
 * If all goes well, a publication for the TFM projects is more than reasonable.
 * A cool demo is always something to be proud of.


# TFG (6 to 8 month projects)

## Project 1: Creation of an Arduino UNO compatible shield for the ProtoCREA robot *(Assigned)*

**Robot**: [[ProtoCREA]]()

**Supervisor**: *Raúl Pérula-Martínez* (UC3M)

**Supervisor**: *Félix Rodríguez Cañadillas* (CREA Robótica Educativa)

{% include image.html exturl="https://upload.wikimedia.org/wikipedia/commons/a/a8/ArduinoUno_R3_Front_450px.jpg" width="width:65%;"  description="Arduino UNO board." %}

### Task 1.1. 

 * 
 
### Task 1.2. 

 * 

### Task 1.3. 

 * 
 
### Task 1.4. 

 * 


## Project 2: Imagination + Engineering: 5 scenarios to teach robotics in the school *(Assigned)*

**Robot**: [[ProtoCREA]]()

**Supervisor**: *Raúl Pérula-Martínez* (UC3M)

**Supervisor**: *Félix Rodríguez Cañadillas* (CREA Robótica Educativa)

{% include image.html url="portfolio/protocrea1.png" width="width:65%;"  description="SnapCircuit board with some parts." %}

### Task 2.1. Perception

 * Automatic Board detection
 * Automatic segmentation of SnapCircuit Parts on the board
 * Automatic recognition of the topology of the board

### Task 2.2. Action

 * Inverse Kinematics for high level control of the end-effector (see [IKFast](https://goo.gl/MwyTXX) or [the SDK Wiki](http://sdk.rethinkrobotics.com/wiki/API_Reference#Inverse_Kinematics_Solver_Service))
 * Manipulation of the SnapCircuit Parts and their placement on the board


# TFM (8 to 12 month projects)

## Project 1: Implementing an integrated people recognizer

**Robot**: *Maggie Research Robot* [[link]]()

**Supervisor**: *Raúl Pérula-Martínez*

{% include video.html url="//www.youtube.com/embed/zegs0TSFF6A" description="State of the art in body representation and control"%}

> *...by 2-3 months, infants engage in exploration of their own body as it moves and acts in the environment. They babble and touch their own body, attracted and actively involved in investigating the rich intermodal redundancies, temporal contingencies, and spatial congruence of self-perception.* [Rochat, 1998]

### TASK 1.1. Kinematic Learning

By starting with as little information as possible, it is possible to compute a reliable enough kinematic model from self-observation and exploration of the consequences of the robot's actions.

> *This process calibrates the robot's kinematic model to its vision model, producing a unified model that allows kinematic and visual information to be meaningfully combined. The presented model is demonstrated to predict end-effector position in the visual field. The robot further refines this combined kinematic-visual model by minimizing the distance between the predicted and observed positions of its end-effector in its visual field. The fact that the robot learns these properties based on first-hand observations of its body, through its sensors, allows the model to adapt to changes in the robot’s configuration on-line. This endows the robot with the ability to adapt its self-model for tool use.* [Hart and Scassellati, 2011]

### References

 * M. Hoffmann et al. [2010]. **Body Schema in Robotics: A Review**. In: *IEEE Transactions on Autonomous Mental Development*. Vol. 2, No. 4, Dec 2010. [[PDF]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.357.6076&rep=rep1&type=pdf)


## Project 2: 

**Robot**: *Maggie Research Robot* [[link]](http://wiki.ros.org/Robots/Maggie)

**Supervisor**: *Raúl Pérula-Martínez*

{% include video.html url="//www.youtube.com/embed/API96d56v3I" %}

### TASK 2.1. Perception

   * Development of a perception algorithm (kinect-based) able to detect obstacles close to the robot's arms
     * Calibration of camera w.r.t. the robot
     * Detection of the robot arms inside the camera frame
     * Detection of robot - obstacle distances throughout the robot's body

### References

 * A. De Luca and F. Flacco [2012]. **Integrated control for pHRI: Collision avoidance, detection, reaction and collaboration**, in *Biomedical Robotics and Biomechatronics (BioRob), 2012 4th IEEE RAS EMBS International Conference on*. June 2012, pp. 288–295. [[PDF]](http://www.dis.uniroma1.it/~labrob/pub/papers/BioRob12.pdf)
